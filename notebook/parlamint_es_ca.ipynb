{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXAnvyee/h758p0Uh7/Wgk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IULATERM-TRL-UPF/ParlaMint_ES-CT/blob/main/notebook/parlamint_es_ca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linguistic annotation of Catalan Parliamentary speeches\n",
        "The Parla-CLARIN proposal involves the linguistic annotation of the parliamentary transcriptions. In this notebook, we present a way to accomplish this annotation given a corpus in XML format.\n",
        "\n",
        "The Catalan Parliament has tow official languages: catalan and spanish. A complete linguistic annotation should include:\n",
        "1. Tokenization and sentence segmentation\n",
        "2. Lemmatization and UD morphological features\n",
        "3. UD syntactic annotations\n",
        "4. NER marking (PER, LOC, ORG, MISC)\n",
        "\n",
        "\n",
        "We conduct the linguistic annotation using Spacy, which offers the feautures listed above, although, it does not identify contractions like \"del\" or \"al\".  This issue may impply that Spacy is not the best option for this task. Nevertheless, we expect that the guidelines exposed here can be used with another NLP library."
      ],
      "metadata": {
        "id": "_jBCuHY2YQyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main Libraries\n",
        "\n",
        "The main libraries applied are:\n",
        "* Spacy for the linguistic annotation, and Spacy_conll to format the result.\n",
        "* xml.etree.ElementTree to parse the XML source file and to create the new XML tree."
      ],
      "metadata": {
        "id": "vwXwcFn_YU7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installation"
      ],
      "metadata": {
        "id": "SYOe9Ji0YazJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5TqIbFeYFVl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/IULATERM-TRL-UPF/ParlaMint_ES-CT\n",
        "!python -m pip install -r ParlaMint_ES-CT/requirements.txt\n",
        "!python -m spacy download ca_core_news_trf\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run Scripts\n",
        "\n",
        "In this section you will test the script to generate the TEI file from the docx file. The TEI file contains the linguistic annotations of the texts of the docx file according to the requirements."
      ],
      "metadata": {
        "id": "8Z-X9N4HUijN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Docx File Folder\n",
        "\n",
        "#@markdown ### In this case you can put \"ParlaMint_ES-CT/samples/\" or another \n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Enter a file path:\n",
        "docx_file_path = \"ParlaMint_ES-CT/samples/\" #@param {type:\"string\"}\n",
        "#@markdown ---"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kTb5RztxS6ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## TEI File Folder\n",
        "\n",
        "#@markdown ### In this case you can put \"ParlaMint_ES-CT/process/\"\" or another \n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Enter a file path:\n",
        "tei_file_path = \"ParlaMint_ES-CT/process/\" #@param {type:\"string\"}\n",
        "#@markdown ---"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Maf75-pDUQEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ParlaMint_ES-CT/src/main.py -i $docx_file_path  -o $tei_file_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIWF9ShHUp-4",
        "outputId": "05dfab9b-fe95-4e2a-8726-fcfaec816a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ParlaMint-ES-CT_2015-11-09-0301.xml', 'ParlaMint-ES-CT_2018-03-01-0201.xml', 'ParlaMint-ES-CT_2018-01-17-0101.xml', 'ParlaMint-ES-CT_2015-10-26-0101.xml', 'ParlaMint-ES-CT_2018-03-22-0301.xml'}\n",
            " 20% 1/5 [08:07<32:29, 487.49s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"ParlaMint_ES-CT/src/main.py\", line 37, in <module>\n",
            "    main()\n",
            "  File \"ParlaMint_ES-CT/src/main.py\", line 31, in main\n",
            "    util.file_creation(os.path.join(root_dataset, a),os.path.join(root_save, a))\n",
            "  File \"/content/ParlaMint_ES-CT/src/util.py\", line 386, in file_creation\n",
            "    generate_tei(seg_seq,seg)\n",
            "  File \"/content/ParlaMint_ES-CT/src/util.py\", line 210, in generate_tei\n",
            "    doc = nlp_ca(text)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy/language.py\", line 1020, in __call__\n",
            "    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy_transformers/pipeline_component.py\", line 192, in __call__\n",
            "    outputs = self.predict([doc])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy_transformers/pipeline_component.py\", line 228, in predict\n",
            "    activations = self.model.predict(docs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/model.py\", line 315, in predict\n",
            "    return self._func(self, X, is_train=False)[0]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/spacy_transformers/layers/transformer_model.py\", line 185, in forward\n",
            "    model_output, bp_tensors = transformer(wordpieces, is_train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/model.py\", line 291, in __call__\n",
            "    return self._func(self, X, is_train=is_train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/layers/pytorchwrapper.py\", line 143, in forward\n",
            "    Ytorch, torch_backprop = model.shims[0](Xtorch, is_train)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/shims/pytorch.py\", line 72, in __call__\n",
            "    return self.predict(inputs), lambda a: ...\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/thinc/shims/pytorch.py\", line 90, in predict\n",
            "    outputs = self._model(*inputs.args, **inputs.kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 858, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 531, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 452, in forward\n",
            "    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\", line 243, in apply_chunking_to_forward\n",
            "    return forward_fn(*input_tensors)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 464, in feed_forward_chunk\n",
            "    layer_output = self.output(intermediate_output, attention_output)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\", line 375, in forward\n",
            "    hidden_states = self.dense(hidden_states)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    }
  ]
}